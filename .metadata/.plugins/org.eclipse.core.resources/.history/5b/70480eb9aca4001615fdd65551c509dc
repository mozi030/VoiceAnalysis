package NeuralNetwork;

import java.util.ArrayList;
import java.util.Arrays;

public class NeuralNetwork {
	final static public int inputNum = 500;
	final static public ArrayList<String> outputs = new ArrayList<String>(
			Arrays.asList("down", "jump", "left", "move", "right", "shoot", "speedup", "up"));
	public static Node allNodes[][];

	final static public int hiddenLayerNum = 2;
	final static public int affectRange = 5;
	final static public float weightInitialValue = 0.01f;
	final static public float learningRate = 0.1f;

	public NeuralNetwork() throws Exception {
		allNodes = new Node[hiddenLayerNum + 2][];
		allNodes[0] = new Node[inputNum];
		//first layer, the input layer
		for (int j = 0; j < inputNum; j++) {
			allNodes[0][j].lastNodeIndexAndWeight = null;
			int begin = j - affectRange / 2;
			if (begin < 0) {
				begin = 0;
			}
			int end = j + affectRange / 2;
			if (end >= inputNum) {
				end = inputNum - 1;
			}
			if (end < begin) {
				throw new Exception("end < begin");
			}
			for (int k = begin; k <= end; k++) {
				allNodes[0][j].nextNodeIndexAndWeight.put(k, weightInitialValue);
			}
		}
		
		//middle layers, the hidden layers
		for (int i = 1; i < allNodes.length - 1; i++) {
			allNodes[i] = new Node[inputNum];
			for(int j = 0; j < inputNum; j++) {
				int begin = j - affectRange / 2;
				if (begin < 0) {
					begin = 0;
				}
				int end = j + affectRange / 2;
				if (end >= inputNum) {
					end = inputNum - 1;
				}
				if (end < begin) {
					throw new Exception("end < begin");
				}
				for (int k = begin; k <= end; k++) {
					allNodes[i][j].lastNodeIndexAndWeight.put(k, weightInitialValue);
				}
				if (i != allNodes.length - 2) {
					for (int k = begin; k <= end; k++) {
						allNodes[i][j].nextNodeIndexAndWeight.put(k, weightInitialValue);
					}
				} else {
					allNodes[i][j].nextNodeIndexAndWeight.put(0, weightInitialValue);
				}
				allNodes.cita = weightInitialValue;
			}
		}
		//last layer, the output layer
		allNodes[allNodes.length - 1] = new Node[1];
		allNodes[allNodes.length - 1][0].nextNodeIndexAndWeight = null;
		for (int k = 0; k < inputNum; k++) {
			allNodes[allNodes.length - 1][0].lastNodeIndexAndWeight.put(k, weightInitialValue);
		}
		


		cita = new float[hiddenLayerNum + 1][];
		for (int i = 0; i < hiddenLayerNum; i++) {
			cita[i] = new float[inputNum];
			Arrays.fill(cita[i], weightInitialValue);
		}
		cita[hiddenLayerNum] = new float[1];
		cita[hiddenLayerNum][0] = weightInitialValue;

		error = new float[hiddenLayerNum + 1][];
		for (int i = 0; i < hiddenLayerNum; i++) {
			error[i] = new float[inputNum];
		}
		error[hiddenLayerNum] = new float[1];

		tempResult = new float[hiddenLayerNum + 1][inputNum];
	}

	public void TrainNetwork(ArrayList<Data> allTrainData) {
		int index = 0;
		// int count = 0;
		while (true) {
			Data currentData = allTrainData.get(index);
			for (int i = 0; i < inputNum; i++) {
				tempResult[0][i] = currentData.dataList[i];
			}
			float evaluatedValue = ForwardFeed(currentData);

			System.out.println("error: " + Math.abs(evaluatedValue - currentData.result));

			BackPropagation(evaluatedValue, currentData.result);
			// if (Math.abs(evaluatedValue - currentData.result) < 1e-3) {
			// count ++;
			// index++;
			// if (index == allTrainData.size()) {
			//
			// }
			// } else {
			//
			// }
		}
	}

	public void BackPropagation(float evaluatedValue, float realResult) {
		error[hiddenLayerNum][0] = evaluatedValue * (1 - evaluatedValue) * (realResult - evaluatedValue);
		for (int i = 0; i < inputNum; i++) {
			weight[hiddenLayerNum][0][i] += learningRate * error[hiddenLayerNum][0] * tempResult[hiddenLayerNum][i];
		}
		cita[hiddenLayerNum][0] += learningRate * error[hiddenLayerNum][0];

		for (int i = hiddenLayerNum - 1; i >= 0; i--) {
			for (int j = 0; j < inputNum; j++) {
				float temp = 0;
				if (error[i + 1].length == 1) {
					temp += weight[hiddenLayerNum][0][j] * error[i + 1][0];
				} else {
					int begin = j - affectRange / 2;
					if (begin < 0) {
						begin = 0;
					}
					int end = j + affectRange / 2;
					if (end >= inputNum) {
						end = inputNum - 1;
					}
					for (int k = begin; k <= end; k++) {

						temp += weight[i + 1][k][k - begin] * error[i + 1][k];
					}
				}
				float o = tempResult[i + 1][j];
				error[i][j] = o * (1 - o) * temp;

				for (int k = 0; k < weight[i][j].length; k++) {
					weight[i][j][k] += learningRate * error[i][j] * tempResult[i][j];
				}
				cita[i][j] += learningRate * error[i][j];
			}
		}
	}

	public float ForwardFeed(Data data) {
		for (int i = 0; i < hiddenLayerNum; i++) {
			for (int j = 0; j < inputNum; j++) {
				float temp = 0;
				int begin = j - affectRange / 2;
				if (begin < 0) {
					begin = 0;
				}
				for (int k = 0; k < weight[i][j].length; k++) {
					temp += weight[i][j][k] * tempResult[i][begin + k];
				}
				temp += cita[i][j];
				tempResult[i + 1][j] = evaluate(temp);
			}
		}
		float temp = 0;
		for (int j = 0; j < inputNum; j++) {
			temp += weight[hiddenLayerNum][0][j] * tempResult[hiddenLayerNum][j];
		}
		temp += cita[hiddenLayerNum][0];
		return evaluate(temp);
	}

	public static float evaluate(float a) {
		return (float) (1.0 / (1 + Math.exp(-a)));
	}
}
